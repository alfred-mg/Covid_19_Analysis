---
title: "Covid !9 Analysis"
author: "Group Work"
date: "2023-11-09"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())

```



```{r, message=FALSE}

# Load required libraries
library(tidyverse)
library(lares) # show the structure of data
library(vtable) # show the mean, sd, min, max
library(caret) 
library(psycho) # Standardizing
library('DescTools')
library(ggcorrplot)
library(plotly) # Using plotly for interactive plots
library(fastDummies) # for dummy encoding

```



## About the Dataset

The data is about Coronavirus (COVID-19) Vaccinations

According to the website, the vaccination dataset uses the most recent official numbers from governments and health ministries worldwide. Population estimates for per-capita metrics are based on the United Nations World Population Prospects. Income groups are based on the World Bank classification. Source: https://ourworldindata.org/covid-vaccinations


**NB: The dataset should have at least 7000 rows and 10 columns after cleaning and there is not any upper
bound. The objectives of the DEP project are based on the domain knowledge of data. Need to complete
the following tasks during the development of this project.**


## Loading the Dataset


```{r}

# loading the data after downloading
data <- read.csv('owid-covid-data.csv')

dim(data)

# overview of the data
head(data, 5)

str(data)

```



### Checking for missing values


```{r, message=FALSE, warning=FALSE}

# Check for null values in the dataframe
is_null <- is.na(data)

# Count the number of null values in each column
num_null_values <- colSums(is_null, na.rm = TRUE)

# Print the number of null values in each column
print(num_null_values)

# show the details of the dataset
df_str(data, return = "plot")

```



As we can see from the above output, there are columns with a very high number of null values. So the first thing would be to drop all the columns whose number of null values is more more than half the total number of rows, or observations.

**Sometimes, R do not read empty strings, and question marks as nulls. So we first convert the Question marks and the empty strings as nulls then check the number of null values.



```{r}

# Replace question marks with NA in the entire dataframe
data[data == "?"] <- NA

# Replace empty strings with NA in the entire dataframe
data[data == ""] <- NA

# Check for null values in each column
null_counts <- colSums(is.na(data))

# Get columns where more than half of the values are null
columns_to_drop <- names(null_counts[null_counts > nrow(data)/2])

# Drop columns with more than half of the values being null
data <- data[, !names(data) %in% columns_to_drop]

dim(data)

# Drop rows with NA values using na.omit()
data <- na.omit(data)

# Check for null values in the dataframe
is_null <- is.na(data)

# Count the number of null values in each column
num_null_values <- colSums(is_null, na.rm = TRUE)

# Print the number of null values in each column
print(num_null_values)

View(data)
```


We can now see that we do not have any null values in the columns.


### Dropping all the Null Values


```{r}

# Drop rows with NA values using na.omit()
data_clean <- na.omit(data)

# checking the shape of the data
cat('The shape of the data:', dim(data_clean)[1], 'rows/observations', 'and', dim(data_clean)[2], 'columns')

```


The Dataset is already eligible for our analysis as it means the eligibility of 7000 and 10 columns




### Removing Unwanted Columns


```{r}

# Removing a single column using subset() function
data <- subset(data, select = -c(population, human_development_index, life_expectancy, 
                                 hospital_beds_per_thousand, male_smokers, female_smokers,
                                 diabetes_prevalence, cardiovasc_death_rate, gdp_per_capita,
                                 aged_70_older, aged_65_older, median_age, population_density,
                                 stringency_index, new_people_vaccinated_smoothed_per_hundred, 
                                 new_vaccinations_smoothed_per_million, new_deaths_smoothed_per_million,
                                 new_deaths_per_million, total_deaths_per_million, new_cases_smoothed_per_million,
                                 new_cases_per_million, total_cases_per_million))

dim(data)
```



### A. Identify which variables are categorical, discrete and continuous in the chosen data set and show using some visualization or plot. Explore whether there are missing values for any of the variables.



```{r}

# Remove all instances of 0 from all columns
data_no_zeros <- data
data_no_zeros[data_no_zeros == 0] <- NA
data <- na.omit(data_no_zeros)

# Check data types of variables
str(data)

```


```{r, warning=FALSE, message=FALSE}

# show the details of the dataset
df_str(data, return = "plot")

```

### Identify which variables are categorical, discrete and continuous in the chosen data set


* Categorical Variables

- iso_code
- continent
- location
- date


* Discrete Variables

- total_cases
- new_cases
- total_deaths
- new_deaths
- new_vaccinations_smoothed 
- new_people_vaccinated_smoothed

* Continous Variables

- new_cases_smoothed
- new_deaths_smoothed
- reproduction_rate



### B. Calculate the statistical parameters (mean, median, minimum, maximum, and standard deviation) for each of the numerical variables. 




```{r}

# Select numerical columns using indexing
numerical_data <- data[sapply(data, is.numeric)]

numerical_data  %>%
      pivot_longer(everything()) %>%
      group_by(name) %>%
       summarise_at(vars(value), list(Mean = mean, Median = median, Min = min,  Max = max, Sd = sd))

# getting the summary statistics
st(numerical_data)

```




### C. Apply Min-Max Normalization, Z-score Standardization and Robust scalar on the numerical data variables.


#### Min-Max Normalization


```{r}

# Apply Min-Max Normalization
preproc <- preProcess(numerical_data, method = c("range"))
scaled_data_minmax <- predict(preproc, numerical_data)
scaled_data_minmax

```



#### Z-score Standardization


```{r}

# Z-score standardization
z_score_standardized_data <- as.data.frame(scale(numerical_data))
print(z_score_standardized_data)

```



#### Robust scalar


```{r}

# Robust Scalar
robust_scalar <- function(x){(x- median(x)) /(quantile(x,probs = .75)-quantile(x,probs = .25))}
robust_scalar_data <- as.data.frame(sapply(numerical_data, robust_scalar))
robust_scalar_data

```



### D. Line, Scatter and Heatmaps can be used to show the correlation between the features of the dataset.


#### Line Plots


#### Total Cases vs Total Deaths


```{r}

# Basic line plot
ggplot(data=data, aes(x=total_cases, y=total_deaths))+
  geom_line(color= 'black') +
  labs(title = "Scatter Plot of Total Cases vs Total Deaths")

```



#### Line Chart of New Deaths and New Deaths Smoothed


```{r}

# Convert 'date' column to Date type if it's not already
data$date <- as.Date(data$date)
# Plot 'new deaths' and 'new deaths smoothed' against dates with different colors for each line
ggplot(data, aes(x = date)) +
  geom_line(aes(y = new_deaths, color = "New deaths")) +
  geom_line(aes(y = new_deaths_smoothed, color = "New deaths smoothed")) +
  scale_color_manual(values = c("New deaths" = "blue", "New deaths smoothed" = "green")) +
  labs(title = "New Deaths and New Deaths Smoothed Over Time", x = "Date", y = "Number of Cases", color = "Legend") +
  theme_minimal()

```




















